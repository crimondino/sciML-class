{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkO_BzBFk3PP"
   },
   "source": [
    "## Scientific Machine Learning 2026, Tutorial 1 part 1:\n",
    "#Learning the Ising Hamiltonian using linear regression\n",
    "\n",
    "Perimeter Scholars International 2025-2026\n",
    "\n",
    "This is a notebook modified from Lauren Hayward's notebook by Gang Xu\n",
    "\n",
    "**References:** Section VI.D of https://arxiv.org/abs/1803.08823, and the corresponding Notebook D from https://physics.bu.edu/~pankajm/MLnotebooks.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0B99Cr8lyO6"
   },
   "source": [
    "### Generate the data set:<span style=\"color:blue\"> (Play with M and N here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jf56CFSllL1u",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn import linear_model\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2026) # we usually set a random seed so whatever we do is repeatable\n",
    "\n",
    "M = 1000  #number of samples\n",
    "N = 20   #number of spins\n",
    "J = 1.0  #coupling energy for generating the data\n",
    "\n",
    "#Randomly generate the spin configurations\n",
    "s = np.random.choice([-1,1], size=(M,N))\n",
    "\n",
    "def getEnergy_nnIsing1D(s):\n",
    "  return -J*np.sum(s*np.roll(s,-1,axis=1),axis=1) \n",
    "    # np.roll shifts the big M by N matrix left by one (-1) and it is applied rowwise(across columns) (axis=1)\n",
    "#s*np.roll elementwise multiplication \n",
    "#np.sum (...axis=1) sum along columns for each row\n",
    "H = getEnergy_nnIsing1D(s) #Labels for each configuration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce the variables X <span style=\"color:blue\"> Fill in the paranthesis of x=np.einsum() part (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Introduce the variable x as in part b): s_j*s_k --> x_p \n",
    "#effectively x[..., j,k]=s[...,j]*s[...,k]\n",
    "#np.einsum('ij,j->i', A,x) will do the matrix multiplication \\sum_jA_{ij}x_{j} sum over repeated index and -> make sure i stay unsummed\n",
    "#if you have a repeated index, but does not want it to be summed over use ... to represent it everywhere\n",
    "#write part c) code here using np.einsum, fill in the paranthesis\n",
    "\n",
    "x=np.einsum()\n",
    "\n",
    "\n",
    "#Consider only the upper triangular part of this matrix since we only want to consider k>j:\n",
    "iu = np.triu_indices(x.shape[1],k=1) #k=1 means strictly above the main diagonal x has dimension (M,N,N) so x.shape[1]=N\n",
    "#triu_indices produces row and column indices for the upper triangle so a 3*3 matrix will return\n",
    "#[0,0,1][1,2,2] representing the pairs 0,1 0,2 1,2\n",
    "print(x.shape)\n",
    "x = x[:,iu[0],iu[1]]#now for each row(M) : take all the elements indexed as upper triangle above\n",
    "print(x.shape) # notice that this process will change the shape of our features from a 3d array to a 2d one which is what we need "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcQe4yugLcWA"
   },
   "source": [
    "### Define a function to plot the coupling parameters $J_{jk}$ <span style=\"color:blue\">(No need to change anything here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "It3VAzdpLm2L",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_J(w,N):\n",
    "  J = np.zeros((N,N))\n",
    "\n",
    "  #Fill in the upper triangular part with entries from w:\n",
    "  iu = np.triu_indices(N,k=1)# same indices as before\n",
    "  J[iu] = -1.0*w\n",
    "  print(np.diag(J,k=1))#k=1 means excluding the diagonal, starts from the first super diagonal\n",
    "\n",
    "  #Plot this matrix:\n",
    "  max_Jmag = max(np.max(J),-1*np.min(J)) # find the max magnitude of J\n",
    "  cmap_args=dict(vmin=min(-1.,-1*max_Jmag), vmax=max(1.,max_Jmag), cmap='PRGn')\n",
    "    #used in imshow below as color mapping parameters vmin is the minimum data value for the colormap, vmax is the maximum data value for the colormap\n",
    "    #ensures the colormap at least cover [-1,1]. notice it centers around 0. PRGn is from purple to green a diverging colormap\n",
    "\n",
    "  fig, ax = plt.subplots()#create figure and axes\n",
    "  im      = ax.imshow(J,**cmap_args)#imshow display a 2D array as an image, each element becomes a pixel values mapped to colors\n",
    "  # it returns an AxesImage object \n",
    "  ax.set_title(r'$J_{jk}$',fontsize=18)\n",
    "\n",
    "  divider = make_axes_locatable(ax)#make divider\n",
    "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)#create a new axes on the right\n",
    "  cbar=fig.colorbar(im, cax=cax)#attach colorbar to this axes\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDKpo_GvbZIg"
   },
   "source": [
    "### Find the parameters $J_{jk}$ using the exact solution <span style=\"color:blue\">(both methods are given, just comment out the one not using) part(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "2aVqbEW9be3V",
    "outputId": "20229af8-48d9-4d21-9988-ed0907921fa9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#w_exact = np.linalg.inv(x.T.dot(x)).dot(x.T).dot(H) #the exact solution\n",
    "w_exact = linear_model.LinearRegression().fit(x, H).coef_ # the exact solution using linear model from sklearn\n",
    "#print(w_exact)\n",
    "plot_J(w_exact,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXkeh4yTurKx"
   },
   "source": [
    "### Find the parameters $J_{jk}$ using gradient descent <span style=\"color:blue\"> (need to code to initialize parameter part e) and remove arguments from the Regressor part g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "q_WnKL3Lvc6a",
    "outputId": "0b09807b-a14e-4fe9-b3dd-ce8cac466c10",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    " # student write code here initialize the w parameters randomly \n",
    "#the shape of w_init should be changing when you change M or N\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Apply gradient descent with a constant learning rate of eta0:\n",
    "#w_GD = linear_model.SGDRegressor(penalty=None, learning_rate='constant', eta0=0.001).fit(x, H, coef_init=w_init).coef_\n",
    "#student write code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_J(w_GD,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.13 (mamba)",
   "language": "python",
   "name": "python313-mamba-module"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
